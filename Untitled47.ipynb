{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77ee0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have econml installed:\n",
    "# pip install econml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# For causal estimation\n",
    "from econml.dr import DRLearner\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c40666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "random.seed(42)\n",
    "\n",
    "###############################################################################\n",
    "# 0. CONFIG & HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "# Let's define some constants and simple helper functions\n",
    "\n",
    "NUM_CUSTOMERS = 600\n",
    "MAX_VISITS_PER_CUSTOMER = 5\n",
    "TOTAL_VISITS_BUDGET = 1200\n",
    "\n",
    "COST_PER_VISIT = 150          # baseline cost per visit\n",
    "REVENUE_PER_CONVERSION = 3000\n",
    "\n",
    "# We'll create a simple function for safe division (to avoid dividing by zero).\n",
    "def safe_div(num, denom):\n",
    "    return num / denom if denom != 0 else 0\n",
    "\n",
    "def expected_conversion_prob(base, sensitivity, diminishing_factor, visits):\n",
    "    \"\"\"A toy function modeling conversion probability given a certain number of visits.\"\"\"\n",
    "    prob = base\n",
    "    for v in range(visits):\n",
    "        prob += sensitivity * (diminishing_factor ** v)\n",
    "    return min(prob, 1.0)\n",
    "\n",
    "###############################################################################\n",
    "# 1. DATA GENERATION\n",
    "###############################################################################\n",
    "# We'll add:\n",
    "#   - territory (categorical)\n",
    "#   - rep_skill (continuous)\n",
    "#   - competitor_calls (int)\n",
    "#   - compliance_limit (int) maximum allowed calls by regulation/policy\n",
    "\n",
    "territories = ['East', 'West', 'North', 'South']\n",
    "customer_data = []\n",
    "\n",
    "for i in range(NUM_CUSTOMERS):\n",
    "    base_potential = random.uniform(0.05, 0.25)\n",
    "    sensitivity = random.uniform(0.10, 0.30)\n",
    "    diminishing_factor = random.uniform(0.7, 0.95)\n",
    "    \n",
    "    # Assign territory randomly\n",
    "    territory = random.choice(territories)\n",
    "    \n",
    "    # Rep skill: float between 0.5 and 1.5, random\n",
    "    rep_skill = random.uniform(0.5, 1.5)\n",
    "    \n",
    "    # Competitor calls: random integer 0..5\n",
    "    competitor_calls = random.randint(0, 5)\n",
    "    \n",
    "    # compliance_limit: we can set it between 3..5 or so\n",
    "    compliance_limit = random.randint(3, 5)\n",
    "    \n",
    "    customer_data.append({\n",
    "        'customer_id': i,\n",
    "        'base_potential': base_potential,\n",
    "        'sensitivity': sensitivity,\n",
    "        'diminishing_factor': diminishing_factor,\n",
    "        'territory': territory,\n",
    "        'rep_skill': rep_skill,\n",
    "        'competitor_calls': competitor_calls,\n",
    "        'compliance_limit': compliance_limit\n",
    "    })\n",
    "\n",
    "df_customers = pd.DataFrame(customer_data)\n",
    "\n",
    "# Because rep skill might amplify or reduce the final conversion probability,\n",
    "# we can incorporate it into the probability function. Let's define\n",
    "# an \"effective\" visits function:\n",
    "def adjusted_conversion_prob(row, visits):\n",
    "    # incorporate rep_skill by boosting \"sensitivity\" a bit\n",
    "    # For example, if rep_skill > 1, we get slightly better effect, etc.\n",
    "    skill_factor = row['rep_skill']\n",
    "    base = row['base_potential']\n",
    "    sens = row['sensitivity'] * skill_factor\n",
    "    dim = row['diminishing_factor']\n",
    "    return expected_conversion_prob(base, sens, dim, visits)\n",
    "\n",
    "# Precompute probability for 0..MAX_VISITS, but also ensure visits <= compliance_limit\n",
    "prob_matrix = {}\n",
    "for i, row in df_customers.iterrows():\n",
    "    prob_matrix[i] = []\n",
    "    for v in range(MAX_VISITS_PER_CUSTOMER + 1):\n",
    "        if v <= row['compliance_limit']:\n",
    "            prob_matrix[i].append(adjusted_conversion_prob(row, v))\n",
    "        else:\n",
    "            # If it exceeds compliance, treat prob as identical to compliance_limit visits\n",
    "            # or we could treat it as invalid, but let's clamp it:\n",
    "            prob_matrix[i].append(adjusted_conversion_prob(row, row['compliance_limit']))\n",
    "\n",
    "# 1.2. Simple Constrained \"Greedy\" Allocation (Optimal Recommendation) \n",
    "# We'll also ensure we don't exceed each customer's compliance_limit.\n",
    "assigned_visits = [0]*NUM_CUSTOMERS\n",
    "remaining_visits = TOTAL_VISITS_BUDGET\n",
    "\n",
    "while remaining_visits > 0:\n",
    "    best_customer = None\n",
    "    best_gain = 0.0\n",
    "    for idx in range(NUM_CUSTOMERS):\n",
    "        current_visits = assigned_visits[idx]\n",
    "        # don't exceed compliance limit or max visits\n",
    "        if current_visits < df_customers.loc[idx, 'compliance_limit'] and current_visits < MAX_VISITS_PER_CUSTOMER:\n",
    "            current_prob = prob_matrix[idx][current_visits]\n",
    "            next_prob = prob_matrix[idx][current_visits+1]\n",
    "            marginal_gain = next_prob - current_prob\n",
    "            if marginal_gain > best_gain:\n",
    "                best_gain = marginal_gain\n",
    "                best_customer = idx\n",
    "    if best_customer is None:\n",
    "        break\n",
    "    assigned_visits[best_customer] += 1\n",
    "    remaining_visits -= 1\n",
    "\n",
    "df_customers['assigned_visits'] = assigned_visits\n",
    "df_customers['optimal_expected_conversion'] = [\n",
    "    prob_matrix[i][assigned_visits[i]] for i in range(NUM_CUSTOMERS)\n",
    "]\n",
    "\n",
    "# 1.3. Actual Visits with Random Deviation\n",
    "actual_visits = []\n",
    "for i in range(NUM_CUSTOMERS):\n",
    "    opt_vis = assigned_visits[i]\n",
    "    draw = random.random()\n",
    "    if draw < 0.5:\n",
    "        # ~50% chance: stick to optimal\n",
    "        v = opt_vis\n",
    "    elif draw < 0.75:\n",
    "        # ~25% chance: 1 less (if possible)\n",
    "        v = max(opt_vis - 1, 0)\n",
    "    else:\n",
    "        # ~25% chance: 1 more (if possible, but don't exceed compliance)\n",
    "        v = min(opt_vis + 1, df_customers.loc[i, 'compliance_limit'])\n",
    "    actual_visits.append(v)\n",
    "\n",
    "df_customers['actual_visits'] = actual_visits\n",
    "\n",
    "# 1.4 Realized Probability + Conversion\n",
    "df_customers['actual_conversion_prob'] = [\n",
    "    prob_matrix[i][df_customers.loc[i, 'actual_visits']] for i in range(NUM_CUSTOMERS)\n",
    "]\n",
    "\n",
    "df_customers['actual_conversion'] = [\n",
    "    1 if random.random() < p else 0 for p in df_customers['actual_conversion_prob']\n",
    "]\n",
    "\n",
    "# 1.5 Cost & Revenue\n",
    "df_customers['cost_optimal'] = df_customers['assigned_visits'] * COST_PER_VISIT\n",
    "df_customers['expected_revenue_optimal'] = df_customers['optimal_expected_conversion'] * REVENUE_PER_CONVERSION\n",
    "df_customers['expected_profit_optimal'] = df_customers['expected_revenue_optimal'] - df_customers['cost_optimal']\n",
    "\n",
    "df_customers['cost_actual'] = df_customers['actual_visits'] * COST_PER_VISIT\n",
    "df_customers['revenue_actual'] = df_customers['actual_conversion'] * REVENUE_PER_CONVERSION\n",
    "df_customers['profit_actual'] = df_customers['revenue_actual'] - df_customers['cost_actual']\n",
    "\n",
    "# 1.6 ROI\n",
    "def safe_roi(profit, cost):\n",
    "    return profit / cost if cost > 0 else 0\n",
    "\n",
    "df_customers['roi_optimal'] = df_customers.apply(\n",
    "    lambda row: safe_roi(row['expected_profit_optimal'], row['cost_optimal']), axis=1\n",
    ")\n",
    "df_customers['roi_actual'] = df_customers.apply(\n",
    "    lambda row: safe_roi(row['profit_actual'], row['cost_actual']), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35d261ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration, let's assume df_customers is already created and has\n",
    "# columns: actual_visits, actual_conversion, assigned_visits, plus features.\n",
    "\n",
    "# Example minimal features subset:\n",
    "X_cols = ['base_potential', 'sensitivity', 'diminishing_factor', \n",
    "          'rep_skill', 'competitor_calls']\n",
    "# Convert territory to dummies if present:\n",
    "df_customers = pd.get_dummies(df_customers, columns=['territory'], drop_first=True)\n",
    "\n",
    "X_cols += [col for col in df_customers.columns if col.startswith('territory_')]\n",
    "\n",
    "# Our final array of features:\n",
    "X = df_customers[X_cols].values\n",
    "\n",
    "# Treatment & Outcome\n",
    "T = df_customers['actual_visits'].values.astype(float)   # continuous or discrete numeric\n",
    "Y = df_customers['actual_conversion'].values.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e8f7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_regression = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_propensity = RandomForestClassifier(n_estimators=100, random_state=42)  # <--- classifier\n",
    "model_final = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "dr_learner = DRLearner(\n",
    "    model_regression=model_regression,\n",
    "    model_propensity=model_propensity,\n",
    "    model_final=model_final,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "check = dr_learner.fit(Y=Y_train, T=T_train, X=X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76664924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_potential_outcome(dr, X, T_val):\n",
    "    \"\"\"\n",
    "    Estimate the potential outcome at a given T_val for each row in X,\n",
    "    using the DRLearner's final stage model + baseline predictions.\n",
    "    \"\"\"\n",
    "    # We'll do: baseline = E[Y|T=0, X], then add effect of going from 0 to T_val.\n",
    "    # effect_0_to_T = dr.const_marginal_effect(X) * T_val \n",
    "    #     if we assume a linear relationship in T, but with random forests it's more complex.\n",
    "    #\n",
    "    # Alternatively, we can do effect(X, T0=0, T1=T_val).\n",
    "    # That gives us the difference in outcomes from T=0 to T=T_val.\n",
    "    \n",
    "    effect_0_to_T = dr.effect(X, T0=np.zeros(X.shape[0]), T1=np.full(X.shape[0], T_val))\n",
    "    \n",
    "    # Now we need E[Y|X, T=0]. DRLearner has a method \"predict\" that can get\n",
    "    # the outcome at T=0 for each X if we do partial dependence. \n",
    "    # However, in many versions, we have to do a small trick:\n",
    "    #   outcome_0 = E[Y|T=0, X] = outcome model - correction from propensity etc.\n",
    "    # We'll approximate by effect(X, T0, T1) approach relative to T=0 baseline.\n",
    "    \n",
    "    # For simplicity, let's do:\n",
    "    #   outcome_0 = dr.effect(X, T0=0, T1=0) + Some baseline?\n",
    "    # Actually, effect(X, T0=0, T1=0) = 0 by definition.\n",
    "    # We'll rely on dr.const_marginal_effect_inference or the internal refit.\n",
    "    \n",
    "    # econml's DRLearner also supports:\n",
    "    #   dr_learner.predict(X, propensity_model=False) for the \"E[Y|T=0,X]\" maybe\n",
    "    # The official doc approach for predicted outcome is:\n",
    "    baseline_0 = dr_learner.model_final_.predict(np.hstack([X, np.zeros((len(X), 1))]))\n",
    "    # That might not fully incorporate the first-stage estimates. \n",
    "    # A more robust path is to do the manual partial dependence approach from the docs.\n",
    "    \n",
    "    # For brevity, let's do a quick approximation: we'll define:\n",
    "    #   outcome_T_val = baseline_0 + effect_0_to_T\n",
    "    # This is a simplification but commonly used in the DR approach. \n",
    "    outcome_T_val = baseline_0 + effect_0_to_T\n",
    "    return outcome_T_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6286e22c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Scenario A: T=0\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m po_0 \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_potential_outcome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Scenario B: T=2 (uniform)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m po_2 \u001b[38;5;241m=\u001b[39m predict_potential_outcome(check, X_test, T_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 29\u001b[0m, in \u001b[0;36mpredict_potential_outcome\u001b[1;34m(dr, X, T_val)\u001b[0m\n\u001b[0;32m     13\u001b[0m effect_0_to_T \u001b[38;5;241m=\u001b[39m dr\u001b[38;5;241m.\u001b[39meffect(X, T0\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), T1\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfull(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], T_val))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Now we need E[Y|X, T=0]. DRLearner has a method \"predict\" that can get\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# the outcome at T=0 for each X if we do partial dependence. \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# However, in many versions, we have to do a small trick:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#   dr_learner.predict(X, propensity_model=False) for the \"E[Y|T=0,X]\" maybe\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# The official doc approach for predicted outcome is:\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m baseline_0 \u001b[38;5;241m=\u001b[39m \u001b[43mdr_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_final_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# That might not fully incorporate the first-stage estimates. \u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# A more robust path is to do the manual partial dependence approach from the docs.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# For brevity, let's do a quick approximation: we'll define:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#   outcome_T_val = baseline_0 + effect_0_to_T\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# This is a simplification but commonly used in the DR approach. \u001b[39;00m\n\u001b[0;32m     36\u001b[0m outcome_T_val \u001b[38;5;241m=\u001b[39m baseline_0 \u001b[38;5;241m+\u001b[39m effect_0_to_T\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:982\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    964\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;124;03m    Predict regression target for X.\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Scenario A: T=0\n",
    "po_0 = predict_potential_outcome(check, X_test, T_val=0)\n",
    "# Scenario B: T=2 (uniform)\n",
    "po_2 = predict_potential_outcome(check, X_test, T_val=2)\n",
    "\n",
    "# Scenario C: each individual gets assigned_visits[i]\n",
    "# We can do it row by row:\n",
    "assigned_list = df_customers.loc[X_test.index, 'assigned_visits'].values\n",
    "po_assigned = []\n",
    "for i, xrow in enumerate(X_test):\n",
    "    tval = assigned_list[i]\n",
    "    eff_0_to_t = dr_learner.effect(xrow.reshape(1, -1), T0=[0.0], T1=[tval])\n",
    "    base_0 = dr_learner.model_final_.predict(np.hstack([xrow.reshape(1, -1), [[0.0]]]))\n",
    "    pred_outcome = base_0 + eff_0_to_t\n",
    "    po_assigned.append(pred_outcome[0])\n",
    "po_assigned = np.array(po_assigned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6710e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'po_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mean_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mpo_0\u001b[49m)\n\u001b[0;32m      2\u001b[0m mean_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(po_2)\n\u001b[0;32m      3\u001b[0m mean_assigned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(po_assigned)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'po_0' is not defined"
     ]
    }
   ],
   "source": [
    "mean_0 = np.mean(po_0)\n",
    "mean_2 = np.mean(po_2)\n",
    "mean_assigned = np.mean(po_assigned)\n",
    "\n",
    "print(f\"Predicted Conversion Rate if T=0 for all:      {mean_0:.3f}\")\n",
    "print(f\"Predicted Conversion Rate if T=2 for all:      {mean_2:.3f}\")\n",
    "print(f\"Predicted Conversion Rate if assigned_visits:  {mean_assigned:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75925da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
